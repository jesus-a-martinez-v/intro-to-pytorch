{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Entrenando Redes Neuronales en PyTorch\n",
    "\n",
    "No importa lo grande, compleja y \"estado del arte\" que sea tu red neuronal: Si no ha entrenado, es bastante probable que se desempeñe pobremente en cualquier conjunto de datos.\n",
    "\n",
    "_Gradient descent_ es un gran algoritmo para disminuir la función de pérdida o error a medida que nos adentramos en el entrenamiento. Recordemos que la manera en la que hacemos que una red neuronal aprenda (o, si al caso vamos, cualquier modelo de machin learning) es mediante la optimización (en este caso, minimización) del error entre las predicciones y los datos reales.\n",
    "\n",
    "Entonces, para una red neuronal de una sola capa, implementar _gradient descent_ realmente no es tan complicado, pero cuando añadimos más y más capas, debemos recurrir a un método más sofisticado conocido como __backpropagation__.\n",
    "\n",
    "Aunque _backpropagation_ está fuera del alcance de este notebook, [aquí hay un recurso](https://codesachin.wordpress.com/2015/12/06/backpropagation-for-dummies/) que me ayudó bastante a entender dicho concepto. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Autograd es un módulo de PyTorch para calcular el gradiente de los tensores. \n",
    "\n",
    "Mantiene un registro de las operaciones ejecutadas sobre ellos.\n",
    "\n",
    "Le podemos indicar a PyTorch que lleve control de los gradientes de varias maneras:\n",
    "  - Al momento de crear un tensor: `my_tensor = torch.ones(10, requires_grad=True)`\n",
    "  - In situ: `my_tensor.requires_grad_(True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminares\n",
    "\n",
    "Con el fin de entender mejor cómo entrenar una red, necesitaremos importar un montón de librerías y paquetes.\n",
    "\n",
    "Hagámoslo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4312, -0.2288],\n",
      "        [-0.8052, -0.2529]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1860, 0.0524],\n",
      "        [0.6483, 0.0640]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x ** 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda de abajo podemos ver la función de derivación (gradiente) que Autograd creó para la operación de potenciación al cuadrado que especificamos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x7fcadc639ba8>\n"
     ]
    }
   ],
   "source": [
    "# Shows the function generated by this variable\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos la media de este tensor y, seguidamente, computemos su gradiente con el método `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2376, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2156, -0.1144],\n",
      "        [-0.4026, -0.1264]])\n",
      "tensor([[ 0.2156, -0.1144],\n",
      "        [-0.4026, -0.1264]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos cálculos de gradientes son la clave para que las redes neuronales aprendan, dado que _backpropagation_ requiere que el error se propague de las últimas hacia las primeras capas del modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo los Datos\n",
    "\n",
    "Aquí descargaremos el conjunto de datos MNIST.\n",
    "\n",
    "También definimos un par de transformaciones a llevar a cabo sobre las imágenes antes de pasárselas a la red. Particularmente, las convertiremos en tensores de PyTorch y después las normalizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_set = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura\n",
    "\n",
    "Aquí es donde definimos la red neuronal como tal.\n",
    "\n",
    "No activaremos las salidas de esta red. En cambio, produciremos _logits_ puros porque producir directamente los resultados de una función _softmax_, la cual genera una distribución de probabilidad sobre las etiquetas, conduce a problemas de inestabilidad numérica ya que los valores están demasiado cerca de 1 o 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "            ('fully_connected_1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "            ('relu_1', nn.ReLU()),\n",
    "            ('fully_connected_2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "            ('relu_2', nn.ReLU()),\n",
    "            ('logits', nn.Linear(hidden_sizes[1], output_size))\n",
    "        ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando las Red\n",
    "\n",
    "Para entrenar la red necesitamos definir la función de pérdida (comúnmente referida como el _criterio_ o, en inglés, _criterion_). \n",
    "\n",
    "Usaremos `nn.CrossEntropyLoss()` porque estamos implementando una salida _softmax_.\n",
    "\n",
    "Para optimizar esta función de pérdida, nos apoyaremos en SGD. Fíjate en que debemos pasar los parámetros que queremos optimizar, los cuales, en el 99% de los casos, serán los de la red que deseamos entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abajo se encuentra el código correspondiente a un solo paso del entrenamiento.\n",
    "\n",
    "Primero, ejecutamos un pase hacia adelante llamadno al método `forward()` sobre el modelo.\n",
    "\n",
    "Después, computamos el error usando nuestro `criterion`. Para ello, le pasamos las etiquetas reales y las salidas de la red.\n",
    "\n",
    "Luego, propagamos hacia atrás el error usando el método `backward()`.\n",
    "\n",
    "Finalmente, actualizamos los parámetros llamando al método `step()` sobre el optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights - Parameter containing:\n",
      "tensor([[ 0.0198, -0.0353,  0.0005,  ...,  0.0129, -0.0194, -0.0167],\n",
      "        [-0.0172, -0.0027, -0.0169,  ...,  0.0315, -0.0208, -0.0124],\n",
      "        [-0.0347,  0.0233, -0.0223,  ...,  0.0299,  0.0345, -0.0140],\n",
      "        ...,\n",
      "        [-0.0257,  0.0205,  0.0093,  ..., -0.0169,  0.0268,  0.0280],\n",
      "        [-0.0260,  0.0152,  0.0153,  ..., -0.0214,  0.0320, -0.0011],\n",
      "        [-0.0149, -0.0354, -0.0199,  ...,  0.0220, -0.0044, -0.0019]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[-0.0012, -0.0012, -0.0012,  ..., -0.0012, -0.0012, -0.0012],\n",
      "        [ 0.0070,  0.0070,  0.0070,  ...,  0.0070,  0.0070,  0.0070],\n",
      "        [ 0.0019,  0.0019,  0.0019,  ...,  0.0019,  0.0019,  0.0019],\n",
      "        ...,\n",
      "        [ 0.0004,  0.0004,  0.0004,  ...,  0.0004,  0.0004,  0.0004],\n",
      "        [ 0.0013,  0.0013,  0.0013,  ...,  0.0013,  0.0013,  0.0013],\n",
      "        [-0.0019, -0.0019, -0.0019,  ..., -0.0019, -0.0019, -0.0019]])\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial weights - {model.fully_connected_1.weight}')\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients. We need to do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward, then update\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print(f'Gradient - {model.fully_connected_1.weight.grad}')\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights - Parameter containing:\n",
      "tensor([[ 0.0198, -0.0352,  0.0005,  ...,  0.0129, -0.0194, -0.0167],\n",
      "        [-0.0173, -0.0028, -0.0170,  ...,  0.0314, -0.0209, -0.0124],\n",
      "        [-0.0348,  0.0233, -0.0224,  ...,  0.0299,  0.0345, -0.0140],\n",
      "        ...,\n",
      "        [-0.0257,  0.0205,  0.0093,  ..., -0.0170,  0.0268,  0.0280],\n",
      "        [-0.0260,  0.0152,  0.0153,  ..., -0.0214,  0.0320, -0.0011],\n",
      "        [-0.0149, -0.0354, -0.0199,  ...,  0.0220, -0.0044, -0.0019]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f'Updated weights - {model.fully_connected_1.weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar de verdad, sólo debemos repetir el paso anterior para cada lote de datos, durante cada epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3...\n",
      "Loss: 2.283420580625534\n",
      "Epoch: 1/3...\n",
      "Loss: 2.2530458986759188\n",
      "Epoch: 1/3...\n",
      "Loss: 2.226108205318451\n",
      "Epoch: 1/3...\n",
      "Loss: 2.2017243444919585\n",
      "Epoch: 1/3...\n",
      "Loss: 2.1648906111717223\n",
      "Epoch: 1/3...\n",
      "Loss: 2.138554239273071\n",
      "Epoch: 1/3...\n",
      "Loss: 2.1004577934741975\n",
      "Epoch: 1/3...\n",
      "Loss: 2.0624050438404082\n",
      "Epoch: 1/3...\n",
      "Loss: 2.015454387664795\n",
      "Epoch: 1/3...\n",
      "Loss: 1.980192482471466\n",
      "Epoch: 1/3...\n",
      "Loss: 1.9096643090248109\n",
      "Epoch: 1/3...\n",
      "Loss: 1.8619372367858886\n",
      "Epoch: 1/3...\n",
      "Loss: 1.8116183131933212\n",
      "Epoch: 1/3...\n",
      "Loss: 1.7427437245845794\n",
      "Epoch: 1/3...\n",
      "Loss: 1.657833382487297\n",
      "Epoch: 1/3...\n",
      "Loss: 1.6017267167568208\n",
      "Epoch: 1/3...\n",
      "Loss: 1.519508996605873\n",
      "Epoch: 1/3...\n",
      "Loss: 1.4497936874628068\n",
      "Epoch: 1/3...\n",
      "Loss: 1.380399578809738\n",
      "Epoch: 1/3...\n",
      "Loss: 1.317969125509262\n",
      "Epoch: 1/3...\n",
      "Loss: 1.2599974572658539\n",
      "Epoch: 1/3...\n",
      "Loss: 1.1765944331884384\n",
      "Epoch: 1/3...\n",
      "Loss: 1.1281732365489006\n",
      "Epoch: 2/3...\n",
      "Loss: 0.6062858313322067\n",
      "Epoch: 2/3...\n",
      "Loss: 1.0808843582868577\n",
      "Epoch: 2/3...\n",
      "Loss: 0.9803182542324066\n",
      "Epoch: 2/3...\n",
      "Loss: 0.9452659338712692\n",
      "Epoch: 2/3...\n",
      "Loss: 0.9355431988835334\n",
      "Epoch: 2/3...\n",
      "Loss: 0.8744421869516372\n",
      "Epoch: 2/3...\n",
      "Loss: 0.8396329432725906\n",
      "Epoch: 2/3...\n",
      "Loss: 0.8332376688718796\n",
      "Epoch: 2/3...\n",
      "Loss: 0.7981594294309616\n",
      "Epoch: 2/3...\n",
      "Loss: 0.7702050119638443\n",
      "Epoch: 2/3...\n",
      "Loss: 0.7366217210888862\n",
      "Epoch: 2/3...\n",
      "Loss: 0.7276996791362762\n",
      "Epoch: 2/3...\n",
      "Loss: 0.700408647954464\n",
      "Epoch: 2/3...\n",
      "Loss: 0.6965779379010201\n",
      "Epoch: 2/3...\n",
      "Loss: 0.6772636011242866\n",
      "Epoch: 2/3...\n",
      "Loss: 0.6648044556379318\n",
      "Epoch: 2/3...\n",
      "Loss: 0.633956378698349\n",
      "Epoch: 2/3...\n",
      "Loss: 0.618031895160675\n",
      "Epoch: 2/3...\n",
      "Loss: 0.6301057383418083\n",
      "Epoch: 2/3...\n",
      "Loss: 0.6083083383738994\n",
      "Epoch: 2/3...\n",
      "Loss: 0.5912940241396427\n",
      "Epoch: 2/3...\n",
      "Loss: 0.6125025264918804\n",
      "Epoch: 2/3...\n",
      "Loss: 0.5936764657497406\n",
      "Epoch: 3/3...\n",
      "Loss: 0.055271728336811064\n",
      "Epoch: 3/3...\n",
      "Loss: 0.5772684156894684\n",
      "Epoch: 3/3...\n",
      "Loss: 0.5527742959558963\n",
      "Epoch: 3/3...\n",
      "Loss: 0.5689622007310391\n",
      "Epoch: 3/3...\n",
      "Loss: 0.5336614094674588\n",
      "Epoch: 3/3...\n",
      "Loss: 0.5223136216402053\n",
      "Epoch: 3/3...\n",
      "Loss: 0.5333709113299847\n",
      "Epoch: 3/3...\n",
      "Loss: 0.509052736312151\n",
      "Epoch: 3/3...\n",
      "Loss: 0.525462044775486\n",
      "Epoch: 3/3...\n",
      "Loss: 0.4935061387717724\n",
      "Epoch: 3/3...\n",
      "Loss: 0.5035738363862038\n",
      "Epoch: 3/3...\n",
      "Loss: 0.49290275350213053\n",
      "Epoch: 3/3...\n",
      "Loss: 0.4947517365217209\n",
      "Epoch: 3/3...\n",
      "Loss: 0.46561753302812575\n",
      "Epoch: 3/3...\n",
      "Loss: 0.4806118182837963\n",
      "Epoch: 3/3...\n",
      "Loss: 0.4703324422240257\n",
      "Epoch: 3/3...\n",
      "Loss: 0.45997221022844315\n",
      "Epoch: 3/3...\n",
      "Loss: 0.46518147438764573\n",
      "Epoch: 3/3...\n",
      "Loss: 0.4442990683019161\n",
      "Epoch: 3/3...\n",
      "Loss: 0.4635390527546406\n",
      "Epoch: 3/3...\n",
      "Loss: 0.45557540133595464\n",
      "Epoch: 3/3...\n",
      "Loss: 0.43890673816204073\n",
      "Epoch: 3/3...\n",
      "Loss: 0.4676941119134426\n",
      "Epoch: 3/3...\n",
      "Loss: 0.46318689472973346\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "print_every = 40\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in iter(train_loader):\n",
    "        steps += 1\n",
    "        \n",
    "        images.resize_(images.size()[0], 784)  # Flatten\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward and backward passes\n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            print(f'Epoch: {epoch + 1}/{epochs}...\\nLoss: {running_loss/print_every}')\n",
    "            \n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fca30172cc0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 227,
       "width": 370
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "image = images[2].view(1, 784)\n",
    "\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(image)\n",
    "    \n",
    "predictions = F.softmax(logits, dim=1)\n",
    "utils.view_classify(image.view(1, 28, 28), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Nuestra red es mucho más inteligente que la que usamos en el notebook anterior!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
